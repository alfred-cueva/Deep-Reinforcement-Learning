{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88af3281",
      "metadata": {
        "id": "88af3281"
      },
      "source": [
        "# HW 4 - Inverse RL & GRPO Fine-Tuning\n",
        "\n",
        "Reward modeling is the glue between human supervision and scalable RL systems. Classical inverse reinforcement learning (IRL) lets us infer reward functions directly from expert demonstrations, and that same idea underpins modern training pipelines (r.g. RLHF, RLAIF, Constitutional AI) where we fit reward models or preference models for large language models. Even though today's top AI labs rely on pairwise/ranked preference data or verifiable rewards rather than tabular MaxEnt IRL, the maximum entropy view is a foundational recipe: it teaches us to match feature expectations while keeping the trajectory distribution as high-entropy (i.e., as uncertain) as possible away from labeled data, which is a philosophy we still enforce when training modern reward models.\n",
        "\n",
        "In **Section 1**, we will apply Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) algorithm from [Ziebart et al., 2008](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf) to recover rewards from demonstrations.\n",
        "\n",
        "In **Section 2**, we will fine-tune a QLoRA-adapted large language model with the Group Relative Policy Optimization (GRPO) algorithm from [Shao et al., 2024](https://arxiv.org/abs/2402.03300) so the LLM outputs follow structured reasoning formats.\n",
        "\n",
        "\n",
        "**Runtime requirement.** This notebook assumes access to a GPU (MaxEnt IRL visualization is light, but GRPO + QLoRA fine-tuning will not run on CPU). If you are using Google Colab, open `Runtime → Change runtime type` and set the hardware accelerator to `GPU`, then restart the runtime before continuing.\n",
        "\n",
        "To conserve GPU hours, we recommend leaving the notebook on a CPU runtime while you work through Section 1 and the early GRPO setup cells. Switch to a GPU only once you reach the GRPO training section where accelerated generation becomes necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660fc4a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "660fc4a8",
        "outputId": "8c0aee70-c7e9-43fc-a9be-c4ecef1ec6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.4.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: math-verify in /usr/local/lib/python3.12/dist-packages (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: latex2sympy2_extended==1.10.2 in /usr/local/lib/python3.12/dist-packages (from math-verify) (1.10.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended==1.10.2->math-verify) (1.14.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime<=4.13.2,>=4.9.3 in /usr/local/lib/python3.12/dist-packages (from latex2sympy2_extended==1.10.2->math-verify) (4.9.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->latex2sympy2_extended==1.10.2->math-verify) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# If running on Colab (recommended)\n",
        "!pip install -U transformers datasets accelerate bitsandbytes peft math-verify"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b937b9",
      "metadata": {
        "id": "14b937b9"
      },
      "source": [
        "## 1. Maximum Entropy IRL [40 pts]\n",
        "\n",
        "1. Conceptual questions [12 pts]\n",
        "2. MaxEnt IRL implementation [28 pts]\n",
        "\n",
        "Maximum Entropy IRL (Ziebart et al., 2008) infers a reward function that explains expert demonstrations while remaining as uncertain as possible beyond the observed behavior. By matching feature expectations under a stochastic policy, we recover rewards that generalize better than directly cloning actions.\n",
        "\n",
        "**Suggested resources.**\n",
        "- B. D. Ziebart et al., *Maximum Entropy Inverse Reinforcement Learning*, AAAI 2008\n",
        "- B. D. Ziebart, *Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy*, PhD thesis, 2010\n",
        "- S. Levine, *Reinforcement Learning and Control as Probabilistic Inference*, NeurIPS 2018 tutorial\n",
        "\n",
        "The exercises that follow rebuild the MaxEnt IRL pipeline from demonstrations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2670749",
      "metadata": {
        "id": "b2670749"
      },
      "source": [
        "### The IRL objective\n",
        "\n",
        "We work with a Markov decision process $(S, A, T, R, \\gamma)$ where transitions $T(s_t, a_t, s_{t+1})$ and the discount $\\gamma$ are known while the per-state reward $R(s)$ is not. Reinforcement learning searches for a policy $\\pi$ that maximizes expected discounted return, but in inverse RL we only watch demonstrations $\\mathcal{D} = \\{\\tau_i\\}$ produced by an expert policy $\\pi^E$. Each trajectory $\\tau = ((s_0, a_0), \\ldots, s_T)$ is summarized through hand-crafted features $\\phi: S \\to \\mathbb{R}^d$, and we assume a linear reward $R_\\omega(s) = \\omega^\\top \\phi(s)$. The goal is to recover parameters $\\omega$ whose induced behavior explains the demonstrations as well as the original expert policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60f195a",
      "metadata": {
        "id": "a60f195a"
      },
      "source": [
        "### Principle of Maximum Entropy\n",
        "\n",
        "Feature-expectation matching requires the learner to visit features just as often as the expert does,\n",
        "$$\n",
        "    \\mathbb{E}_{\\pi^L}[\\phi(\\tau)] = \\mathbb{E}_{\\pi^E}[\\phi(\\tau)],\n",
        "$$\n",
        "where the learner's policy $\\pi^L$ emerges from the trajectory distribution $p(\\tau)$. This constraint alone leaves infinitely many rewards that can explain $\\mathcal{D}$. Following Jaynes (1957) and Ziebart et al. (2008), we pick the solution with maximum entropy so that we add no extra bias beyond the data we observed. Intuitively, higher-entropy distributions encode fewer additional assumptions—if two policies both satisfy the feature constraints, we prefer the one whose trajectories remain as uncertain as possible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e6fd9a",
      "metadata": {
        "id": "71e6fd9a"
      },
      "source": [
        "### Trajectory distributions, gradients, and state visitation frequencies\n",
        "\n",
        "Applying Lagrange multipliers yields a trajectory distribution of the form\n",
        "$$\n",
        "    p(\\tau \\mid \\omega) = \\frac{1}{Z(\\omega)} \\exp(\\omega^\\top \\phi(\\tau)) \\prod_{t} p(s_{t+1} \\mid s_t, a_t),\n",
        "$$\n",
        "with partition function $Z(\\omega)$ ensuring normalization. Maximizing the log-likelihood of the demonstrations leads to the gradient\n",
        "$$\n",
        "    \\nabla_\\omega \\mathcal{L}(\\omega) = \\mu_E - \\sum_s D_s\\, \\phi(s),\n",
        "$$\n",
        "where $\\mu_E$ is the empirical feature expectation and $D_s$ is the time-aggregated expected state-visitation frequency (sum over all time steps) under the current reward. In the forward recursion below we will write $d_t(s)$ for the per-time-step visitation frequency at horizon index $t$, so that $D_s = \\sum_t d_t(s)$. Computing $D_s$ requires a backward pass for the partition functions ($Z_{s,a}$ and $Z_s$ provide the local action probabilities $\\pi(a \\mid s) = Z_{s,a} / Z_s$) followed by a forward rollout that propagates visitation counts from the initial-state distribution. These are the exact steps implemented below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50651d83",
      "metadata": {
        "id": "50651d83"
      },
      "source": [
        "### 1.a Conceptual Questions [12 pts total; 3 pts each]\n",
        "\n",
        "1. Two demonstrations $\\tau_1$ and $\\tau_2$ accumulate feature sums $F_1$ and $F_2$. Under the MaxEnt IRL distribution, write the log-odds $\\log p(\\tau_1 \\mid \\omega) - \\log p(\\tau_2 \\mid \\omega)$ and state when the odds become zero.\n",
        "<font color=\"blue\">Since $p(\\tau \\mid \\omega) = \\frac{1}{Z(\\omega)} \\exp(\\omega^\\top \\phi(\\tau)) \\prod_t p(s_{t+1} \\mid s_t, a_t)$, the log-odds are:\n",
        "$$\\log p(\\tau_1 \\mid \\omega) - \\log p(\\tau_2 \\mid \\omega) = \\omega^\\top \\phi(\\tau_1) - \\omega^\\top \\phi(\\tau_2) = \\omega^\\top (F_1 - F_2)$$\n",
        "The partition function $Z(\\omega)$ and dynamics cancel out. The odds become zero (equal probability) when $\\omega^\\top F_1 = \\omega^\\top F_2$, i.e., when the two trajectories have identical total reward under the current parameters.</font>\n",
        "\n",
        "2. Explain what goes wrong if you try to roll out expected visitation frequencies before finishing the backward partition pass.\n",
        "<font color=\"blue\">The forward visitation recursion requires the soft-optimal policy $\\pi(a \\mid s) = Z_{s,a} / Z_s$ to propagate probability mass. Without completing the backward pass to compute the partition functions $Z_s$ and $Z_{s,a}$, we have no policy to determine the action distribution at each state, making the forward rollout impossible.</font>\n",
        "\n",
        "3. Provide the forward recursion for the expected visitation frequencies $d_{t+1}(s')$ **and** state how you prevent terminal states from re-emitting probability mass.\n",
        "<font color=\"blue\">The forward recursion is:\n",
        "$$d_{t+1}(s') = \\sum_{s,a} d_t(s)\\,\\pi(a\\mid s)\\,P(s'\\mid s,a)$$\n",
        "To prevent terminal states from re-emitting probability mass, we skip the propagation step when $s$ is in the terminal set—terminal states should not initiate any outgoing transitions in the forward pass.</font>\n",
        "\n",
        "4. Name one numerical-stability technique for the partition or visitation computations and briefly describe how it mitigates underflow/overflow.\n",
        "<font color=\"blue\">Log-space computation with the log-sum-exp trick. Instead of directly computing $\\exp(r(s))$ and partition sums which can overflow/underflow, we work in log-space: $\\log Z = \\log \\sum_i \\exp(x_i) = \\max_i x_i + \\log \\sum_i \\exp(x_i - \\max_i x_i)$. This keeps intermediate values numerically stable by preventing both very large exponentials (overflow) and very small probabilities (underflow).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68269765",
      "metadata": {
        "id": "68269765"
      },
      "source": [
        "### 1.b MaxEnt IRL Implementation [28 pts]\n",
        "\n",
        "Implement the four helpers below (feature expectations, initial-state distribution, expected state visitation frequencies, and the exponentiated-gradient solver). Section 1 unit tests check them in the same order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb1a9d4",
      "metadata": {
        "id": "fdb1a9d4"
      },
      "source": [
        "### Section 1 implementation roadmap\n",
        "\n",
        "The next three code cells are boilerplate: they define the GridWorld dynamics, trajectory containers, and data-generation utilities shared by this notebook. Skim them for context, but there is no TODO or grading logic inside.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73ffbce",
      "metadata": {
        "id": "f73ffbce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from itertools import product\n",
        "from typing import Callable, Iterable, List\n",
        "\n",
        "# -------------------------------\n",
        "# Grid world dynamics\n",
        "# -------------------------------\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size: int):\n",
        "        self.size = size\n",
        "        self.actions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
        "        self.n_actions = len(self.actions)\n",
        "        self.n_states = size * size\n",
        "        self.p_transition = self._build_transition_table()\n",
        "\n",
        "    def state_index_to_point(self, state: int):\n",
        "        return state % self.size, state // self.size\n",
        "\n",
        "    def state_point_to_index(self, point):\n",
        "        x, y = point\n",
        "        return y * self.size + x\n",
        "\n",
        "    def _clip_point(self, point):\n",
        "        x, y = point\n",
        "        x = min(max(x, 0), self.size - 1)\n",
        "        y = min(max(y, 0), self.size - 1)\n",
        "        return x, y\n",
        "\n",
        "    def _step_intended(self, state: int, action: int):\n",
        "        x, y = self.state_index_to_point(state)\n",
        "        dx, dy = self.actions[action]\n",
        "        return self.state_point_to_index(self._clip_point((x + dx, y + dy)))\n",
        "\n",
        "    def _build_transition_table(self):\n",
        "        table = np.zeros((self.n_states, self.n_states, self.n_actions), dtype=np.float32)\n",
        "        for s_from, s_to, a in product(range(self.n_states), range(self.n_states), range(self.n_actions)):\n",
        "            table[s_from, s_to, a] = self._transition_prob(s_from, s_to, a)\n",
        "        return table\n",
        "\n",
        "    def _transition_prob(self, s_from, s_to, a):\n",
        "        fx, fy = self.state_index_to_point(s_from)\n",
        "        tx, ty = self.state_index_to_point(s_to)\n",
        "        ax, ay = self.actions[a]\n",
        "\n",
        "        # deterministic transition defined by action\n",
        "        if fx + ax == tx and fy + ay == ty:\n",
        "            return 1.0\n",
        "\n",
        "        # we can stay at the same state if we would move over an edge\n",
        "        if fx == tx and fy == ty:\n",
        "            if not 0 <= fx + ax < self.size or not 0 <= fy + ay < self.size:\n",
        "                return 1.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "class IcyGridWorld(GridWorld):\n",
        "    def __init__(self, size: int, p_slip: float):\n",
        "        self.p_slip = p_slip\n",
        "        super().__init__(size)\n",
        "\n",
        "    def _transition_prob(self, s_from, s_to, a):\n",
        "        fx, fy = self.state_index_to_point(s_from)\n",
        "        tx, ty = self.state_index_to_point(s_to)\n",
        "        ax, ay = self.actions[a]\n",
        "\n",
        "        # intended transition defined by action\n",
        "        if fx + ax == tx and fy + ay == ty:\n",
        "            return 1.0 - self.p_slip + self.p_slip / self.n_actions\n",
        "\n",
        "        # we can slip to all neighboring states\n",
        "        if abs(fx - tx) + abs(fy - ty) == 1:\n",
        "            return self.p_slip / self.n_actions\n",
        "\n",
        "        # we can stay at the same state if we would move over an edge\n",
        "        if fx == tx and fy == ty:\n",
        "            if not 0 <= fx + ax < self.size or not 0 <= fy + ay < self.size:\n",
        "                if not 0 < fx < self.size - 1 and not 0 < fy < self.size - 1:\n",
        "                    return 1.0 - self.p_slip + 2.0 * self.p_slip / self.n_actions\n",
        "                return 1.0 - self.p_slip + self.p_slip / self.n_actions\n",
        "\n",
        "            if not 0 < fx < self.size - 1 and not 0 < fy < self.size - 1:\n",
        "                return 2.0 * self.p_slip / self.n_actions\n",
        "            if not 0 < fx < self.size - 1 or not 0 < fy < self.size - 1:\n",
        "                return self.p_slip / self.n_actions\n",
        "            return 0.0\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def state_features(world: GridWorld) -> np.ndarray:\n",
        "    return np.eye(world.n_states, dtype=np.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4e514d",
      "metadata": {
        "id": "bf4e514d"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Trajectories and solvers\n",
        "# -------------------------------\n",
        "\n",
        "@dataclass\n",
        "class Transition:\n",
        "    s_from: int\n",
        "    action: int\n",
        "    s_to: int\n",
        "\n",
        "\n",
        "class Trajectory:\n",
        "    def __init__(self, transitions: List[Transition]):\n",
        "        self._transitions = transitions\n",
        "\n",
        "    def transitions(self) -> List[Transition]:\n",
        "        return self._transitions\n",
        "\n",
        "    def state_sequence(self) -> np.ndarray:\n",
        "        states = [tr.s_from for tr in self._transitions]\n",
        "        if self._transitions:\n",
        "            states.append(self._transitions[-1].s_to)\n",
        "        return np.asarray(states, dtype=np.int64)\n",
        "\n",
        "\n",
        "def generate_trajectory(world: GridWorld, policy_logits: np.ndarray, start: int, terminal: Iterable[int], rng: np.random.Generator) -> Trajectory:\n",
        "    state = start\n",
        "    transitions = []\n",
        "    while state not in terminal:\n",
        "        probs = policy_logits[state]\n",
        "        action = rng.choice(world.n_actions, p=probs)\n",
        "        next_state = rng.choice(world.n_states, p=world.p_transition[state, :, action])\n",
        "        transitions.append(Transition(state, action, next_state))\n",
        "        state = next_state\n",
        "    return Trajectory(transitions)\n",
        "\n",
        "\n",
        "def generate_trajectories(world: GridWorld, policy_logits: np.ndarray, start_states: np.ndarray, terminal: Iterable[int], n_trajectories: int, seed: int = 0) -> List[Trajectory]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    demos = []\n",
        "    for _ in range(n_trajectories):\n",
        "        s0 = rng.choice(start_states)\n",
        "        demos.append(generate_trajectory(world, policy_logits, s0, terminal, rng))\n",
        "    return demos\n",
        "\n",
        "\n",
        "def value_iteration(p_transition: np.ndarray, reward: np.ndarray, discount: float, eps: float = 1e-4) -> np.ndarray:\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "    v = np.zeros(n_states, dtype=np.float32)\n",
        "    delta = np.inf\n",
        "    while delta > eps:\n",
        "        v_old = v.copy()\n",
        "        q = np.zeros((n_states, n_actions), dtype=np.float32)\n",
        "        for a in range(n_actions):\n",
        "            q[:, a] = reward + discount * p_transition[:, :, a] @ v\n",
        "        v = q.max(axis=1)\n",
        "        delta = np.max(np.abs(v - v_old))\n",
        "    return v\n",
        "\n",
        "\n",
        "def stochastic_policy_from_value(world: GridWorld, value: np.ndarray, weighting: Callable[[float], float] = lambda x: x) -> np.ndarray:\n",
        "    softened = np.exp(value.astype(np.float64))\n",
        "    prefs = np.zeros((world.n_states, world.n_actions), dtype=np.float64)\n",
        "    for s in range(world.n_states):\n",
        "        for a in range(world.n_actions):\n",
        "            intended = world._step_intended(s, a)\n",
        "            prefs[s, a] = weighting(softened[intended])\n",
        "    row_sums = prefs.sum(axis=1, keepdims=True)\n",
        "    row_sums[row_sums == 0.0] = 1.0\n",
        "    return (prefs / row_sums).astype(np.float32)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb7747e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edb7747e",
        "outputId": "89de245d-46e8-4827-d749-9a8dfcc85356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "World: 5x5, trajectories: 200, feature dim: 25\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# Problem setup\n",
        "# -------------------------------\n",
        "\n",
        "def setup_training_world():\n",
        "    world = IcyGridWorld(size=5, p_slip=0.2)\n",
        "    reward = np.zeros(world.n_states, dtype=np.float32)\n",
        "    reward[-1] = 1.0\n",
        "    reward[8] = 0.65\n",
        "    terminal = np.array([world.n_states - 1], dtype=np.int64)\n",
        "    return world, reward, terminal\n",
        "\n",
        "\n",
        "def generate_expert_data(world, reward, terminal, discount=0.9, n_trajectories=200, seed=11, weighting_exp=50):\n",
        "    value = value_iteration(world.p_transition, reward, discount)\n",
        "    policy = stochastic_policy_from_value(world, value, weighting=lambda v: v ** weighting_exp)\n",
        "    start_states = np.array([0], dtype=np.int64)\n",
        "    return generate_trajectories(world, policy, start_states, terminal, n_trajectories, seed)\n",
        "\n",
        "\n",
        "world, expert_reward, terminal_states = setup_training_world()\n",
        "features = state_features(world)\n",
        "expert_trajectories = generate_expert_data(world, expert_reward, terminal_states)\n",
        "print(f\"World: {world.size}x{world.size}, trajectories: {len(expert_trajectories)}, feature dim: {features.shape[1]}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6fcdca",
      "metadata": {
        "id": "0c6fcdca"
      },
      "source": [
        "Recall that we estimate the demonstration features via\n",
        "$$\\hat{\\mu}_E = \\frac{1}{N} \\sum_{i=1}^N \\sum_t \\phi(s_t^{(i)}).$$\n",
        "The helper just accumulates feature vectors along each valid trajectory and averages across the demos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f277aa",
      "metadata": {
        "id": "72f277aa"
      },
      "outputs": [],
      "source": [
        "def feature_expectation_from_trajectories(features: np.ndarray, demos: Iterable[Trajectory]) -> np.ndarray:\n",
        "    totals = np.zeros(features.shape[1], dtype=np.float64)\n",
        "    n_trajectories = 0\n",
        "    for traj in demos:\n",
        "        visited = traj.state_sequence()\n",
        "        if visited.size == 0:\n",
        "            continue\n",
        "        # student code here\n",
        "        # TODO: sum the feature vectors for visited states and keep count of valid demos\n",
        "        totals += features[visited].sum(axis=0)\n",
        "        n_trajectories += 1\n",
        "        # end student code\n",
        "    return totals / n_trajectories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55e5cfd",
      "metadata": {
        "id": "b55e5cfd"
      },
      "outputs": [],
      "source": [
        "def initial_probabilities_from_trajectories(n_states: int, demos: Iterable[Trajectory]) -> np.ndarray:\n",
        "    counts = np.zeros(n_states, dtype=np.float64)\n",
        "    for traj in demos:\n",
        "        trans = traj.transitions()\n",
        "        if not trans:\n",
        "            continue\n",
        "        counts[trans[0].s_from] += 1.0\n",
        "    # student code here\n",
        "    # TODO: normalize the start-state histogram into a valid probability distribution\n",
        "    counts = counts / counts.sum()\n",
        "    # end student code\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d70a0c",
      "metadata": {
        "id": "a2d70a0c"
      },
      "source": [
        "#### Backward partitions and expected visitation frequencies\n",
        "\n",
        "This funciton implements the MaxEnt IRL recursion:\n",
        "$$Z_{s,a} = e^{r(s)} \\sum_{s'} P(s'\\mid s,a) Z_{s'} , \\qquad Z_s = \\sum_a Z_{s,a}.$$\n",
        "Once the policy $\\pi(a\\mid s) = Z_{s,a} / Z_s$ is available, push visitation counts with\n",
        "$$d_{t+1}(s') = \\sum_{s,a} d_t(s)\\,\\pi(a\\mid s)\\,P(s'\\mid s,a).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84deef7",
      "metadata": {
        "id": "d84deef7"
      },
      "outputs": [],
      "source": [
        "def compute_expected_svf(p_transition: np.ndarray, p_initial: np.ndarray, terminal: Iterable[int], reward: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
        "    n_states, _, n_actions = p_transition.shape\n",
        "    terminal = set(int(t) for t in terminal)\n",
        "\n",
        "    Zs = np.zeros(n_states, dtype=np.float64)\n",
        "    Zs[list(terminal)] = 1.0\n",
        "    exp_reward = np.exp(reward)\n",
        "\n",
        "    for _ in range(2 * n_states):\n",
        "        Za = np.zeros((n_states, n_actions), dtype=np.float64)\n",
        "        for s in range(n_states):\n",
        "            for a in range(n_actions):\n",
        "                # student code here\n",
        "                # TODO: compute the state-action partition value using the backward recursion\n",
        "                Za[s, a] = exp_reward[s] * (p_transition[s, :, a] @ Zs)\n",
        "                # end student code\n",
        "        new_Zs = Za.sum(axis=1)\n",
        "        if np.max(np.abs(new_Zs - Zs)) < eps:\n",
        "            Zs = new_Zs\n",
        "            break\n",
        "        Zs = new_Zs\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        policy = np.divide(Za, Zs[:, None], out=np.zeros_like(Za), where=Zs[:, None] > 0)\n",
        "\n",
        "    horizon = 2 * n_states\n",
        "    d = np.zeros((n_states, horizon), dtype=np.float64)\n",
        "    d[:, 0] = p_initial\n",
        "    for t in range(1, horizon):\n",
        "        for s_prev in range(n_states):\n",
        "            if s_prev in terminal:\n",
        "                continue\n",
        "            for a in range(n_actions):\n",
        "                prob = d[s_prev, t - 1] * policy[s_prev, a]\n",
        "                if prob == 0:\n",
        "                    continue\n",
        "                # student code here\n",
        "                # TODO: push visitation probability mass forward through the transition model\n",
        "                d[:, t] += prob * p_transition[s_prev, :, a]\n",
        "                # end student code\n",
        "    return d.sum(axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4e78e2",
      "metadata": {
        "id": "3a4e78e2"
      },
      "source": [
        "#### Exponentiated-gradient MaxEnt IRL update\n",
        "\n",
        "At each iteration we compute the gradient\n",
        "$$\\nabla_\\omega = \\mu_E - \\mu_{\\pi_\\omega}$$\n",
        "and apply an exponentiated-gradient step\n",
        "$$\\omega \\leftarrow \\omega \\odot \\exp(\\alpha_t \\nabla_\\omega), \\qquad \\alpha_t = \\frac{\\alpha}{1 + t}.$$\n",
        "This keeps feature weights positive while shrinking the step size over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3a06d2",
      "metadata": {
        "id": "9a3a06d2"
      },
      "outputs": [],
      "source": [
        "def maxent_irl(p_transition: np.ndarray, features: np.ndarray, terminal: Iterable[int], demos: Iterable[Trajectory], lr: float = 0.2, eps: float = 1e-4, max_iter: int = 1000) -> np.ndarray:\n",
        "    demos = list(demos)\n",
        "    empirical = feature_expectation_from_trajectories(features, demos)\n",
        "    p_initial = initial_probabilities_from_trajectories(p_transition.shape[0], demos)\n",
        "\n",
        "    omega = np.ones(features.shape[1], dtype=np.float64)\n",
        "    step = 0\n",
        "    delta = np.inf\n",
        "    while delta > eps and step < max_iter:\n",
        "        omega_old = omega.copy()\n",
        "        reward = features @ omega\n",
        "        expected_svf = compute_expected_svf(p_transition, p_initial, terminal, reward)\n",
        "        model_features = features.T @ expected_svf\n",
        "        # student code here\n",
        "        # TODO: form the feature-expectation gradient and apply an exponentiated-gradient ascent step with a decaying learning rate\n",
        "        grad = empirical - model_features\n",
        "        alpha_t = lr / (1 + step)\n",
        "        omega = omega * np.exp(alpha_t * grad)\n",
        "        # end student code\n",
        "        delta = np.max(np.abs(omega - omega_old))\n",
        "        step += 1\n",
        "    learned_reward = features @ omega\n",
        "    return learned_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b5fce54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b5fce54",
        "outputId": "17f616e4-5cdb-4356-9b5a-dbe3eb3a69b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Feature expectations: max |Δ|=0.00e+00\n",
            "[OK] Initial-state histogram: sums to 1.0\n",
            "[OK] Initial-state histogram entries: max |Δ|=0.00e+00\n",
            "[OK] Expected SVF sanity check: max |Δ|=0.00e+00\n",
            "Max feature expectation gap: 0.8080\n",
            "Section 1 tests passed.\n"
          ]
        }
      ],
      "source": [
        "# Tests\n",
        "\n",
        "def assert_close(name: str, actual, expected, atol=1e-6):\n",
        "    actual = np.asarray(actual, dtype=float)\n",
        "    expected = np.asarray(expected, dtype=float)\n",
        "    diff = float(np.max(np.abs(actual - expected)))\n",
        "    if diff > atol:\n",
        "        raise AssertionError(f\"{name} mismatch (max |Δ|={diff:.3e}). Expected {expected}, got {actual}\")\n",
        "    print(f\"[OK] {name}: max |Δ|={diff:.2e}\")\n",
        "\n",
        "\n",
        "def assert_probability_vector(name: str, vec):\n",
        "    total = float(np.sum(vec))\n",
        "    if not np.isclose(total, 1.0, atol=1e-6):\n",
        "        raise AssertionError(f\"{name} should sum to 1.0 but sums to {total:.6f}\")\n",
        "    if np.any(vec < -1e-8):\n",
        "        raise AssertionError(f\"{name} contains negative mass: {vec}\")\n",
        "    print(f\"[OK] {name}: sums to 1.0\")\n",
        "\n",
        "\n",
        "simple_features = np.eye(3)\n",
        "traj_a = Trajectory([Transition(0, 0, 1), Transition(1, 0, 2)])\n",
        "traj_b = Trajectory([Transition(0, 0, 0), Transition(0, 0, 1)])\n",
        "fe = feature_expectation_from_trajectories(simple_features, [traj_a, traj_b])\n",
        "assert_close(\"Feature expectations\", fe, np.array([1.5, 1.0, 0.5]))\n",
        "\n",
        "pi0 = initial_probabilities_from_trajectories(3, [traj_a, traj_b])\n",
        "assert_probability_vector(\"Initial-state histogram\", pi0)\n",
        "assert_close(\"Initial-state histogram entries\", pi0, np.array([1.0, 0.0, 0.0]))\n",
        "\n",
        "p_chain = np.zeros((3, 3, 1))\n",
        "p_chain[0, 1, 0] = 1.0\n",
        "p_chain[1, 2, 0] = 1.0\n",
        "p_chain[2, 2, 0] = 1.0\n",
        "svf = compute_expected_svf(p_chain, np.array([1.0, 0.0, 0.0]), [2], np.zeros(3))\n",
        "assert_close(\"Expected SVF sanity check\", svf, np.array([1.0, 1.0, 1.0]))\n",
        "\n",
        "learned_reward = maxent_irl(world.p_transition, features, terminal_states, expert_trajectories)\n",
        "p_initial_world = initial_probabilities_from_trajectories(world.n_states, expert_trajectories)\n",
        "svf_world = compute_expected_svf(world.p_transition, p_initial_world, terminal_states, learned_reward)\n",
        "empirical_features = feature_expectation_from_trajectories(features, expert_trajectories)\n",
        "model_features = features.T @ svf_world\n",
        "max_feature_diff = float(np.max(np.abs(model_features - empirical_features)))\n",
        "print(f\"Max feature expectation gap: {max_feature_diff:.4f}\")\n",
        "if max_feature_diff > 0.85:\n",
        "    raise AssertionError(\n",
        "        \"Learned reward does not yet match demonstration feature expectations closely enough.\"\n",
        "    )\n",
        "print(\"Section 1 tests passed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbe5227",
      "metadata": {
        "id": "fbbe5227"
      },
      "source": [
        "#### Visualizing learned rewards and policies\n",
        "After training, we can visualize the learned reward function and the induced policy. These plots illustrate how well the IRL algorithm has captured the expert's behavior through the inferred reward structure. Your estimated rewards should have the correct top two high reward states, but expect some noise in the lower-value states due to limited demonstration data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ac783d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "b1ac783d",
        "outputId": "4e874ef4-2951-4639-bb1e-6557e0c7873a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_reward_comparison(world, expert_reward, learned_reward):\n",
        "    side = world.size\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    vmin = min(expert_reward.min(), learned_reward.min())\n",
        "    vmax = max(expert_reward.max(), learned_reward.max())\n",
        "    im0 = axes[0].imshow(expert_reward.reshape(side, side), cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
        "    axes[0].set_title(\"Expert Reward\")\n",
        "    axes[1].imshow(learned_reward.reshape(side, side), cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
        "    axes[1].set_title(\"Learned Reward\")\n",
        "    for ax in axes:\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    fig.colorbar(im0, ax=axes.ravel().tolist(), shrink=0.75)\n",
        "    plt.show()\n",
        "\n",
        "plot_reward_comparison(world, expert_reward, learned_reward)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a12e551",
      "metadata": {
        "id": "9a12e551"
      },
      "source": [
        "## 2. GRPO Fine-Tuning from Scratch [40 pts]\n",
        "\n",
        "1. Conceptual questions [10 pts]\n",
        "2. GRPO implementation [30 pts]\n",
        "\n",
        "Group Relative Policy Optimization (GRPO) extends PPO for RLHF by sampling multiple responses per prompt and using their mean reward as a control variate. This stabilizes policy updates when rewards come from evaluators or heuristics--exactly the setting for aligning language and multimodal models.\n",
        "\n",
        "**Suggested resources.**\n",
        "- Zhihong Shao et al., *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*, 2024 (introduces GRPO)\n",
        "- Hugging Face TRL blog, *GRPO: Group Relative Policy Optimization*\n",
        "- lsdefine/simple_GRPO GitHub repository (minimal open-source implementation)\n",
        "\n",
        "The remaining cells guide you through implementing the reward, advantage, and loss components so the GRPO training loop actually runs end-to-end.\n",
        "\n",
        "**GRPO recap (Shao et al., 2024).** For each prompt $i$ we draw a group of $m$ completions $\\{y_{i,j}\\}_{j=1}^m$ from the policy. Rewards $r_{i,j}$ are centered with the group mean $\\bar r_i = \\tfrac{1}{m}\\sum_j r_{i,j}$ to form advantages $A_{i,j} = r_{i,j} - \\bar r_i$. The policy update maximizes the PPO-style surrogate $\\min\\big(r_{i,j}A_{i,j}, \\operatorname{clip}(r_{i,j}, 1 \\pm \\epsilon)A_{i,j}\\big)$ while subtracting a KL penalty $\\beta\\,\\mathrm{KL}(\\pi_\\theta \\Vert \\pi_{\\text{ref}})$ to keep the fine-tuned model close to the frozen reference. This notebook isolates each mathematical piece so you can test it in isolation before chaining them together.\n",
        "\n",
        "**What you are implementing.** You will (a) code the structural reward checker, (b) normalize group advantages, (c) derive the GRPO loss from the log-prob ratios, and (d) plug everything into a gradient-accumulated training loop that mirrors Algorithm 1 in the paper. The surrounding evaluation cells verify that format adherence improves after RL fine-tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13bb158d",
      "metadata": {
        "id": "13bb158d"
      },
      "source": [
        "### 2.a Conceptual Questions [10 pts total; 2 pts each]\n",
        "\n",
        "1. Why does GRPO draw $G>1$ completions per prompt, and what is the prompt-level control variate used to center rewards?\n",
        "<font color=\"blue\">GRPO draws $G>1$ completions per prompt to obtain multiple samples from the same input, which enables variance reduction through the control variate. The prompt-level control variate is the group mean reward: $\\bar{r}_i = \\frac{1}{G}\\sum_{j=1}^G r_{i,j}$, which is subtracted from each individual reward to center the advantages within each group.</font>\n",
        "\n",
        "2. A prompt yields rewards $(2, 1, -1)$ with group size $G=3$. Compute the centered advantages for each completion.\n",
        "<font color=\"blue\">Mean: $\\bar{r} = (2 + 1 + (-1))/3 = 2/3$\n",
        "Standard deviation: $\\sigma = \\sqrt{\\frac{1}{3}[(2-2/3)^2 + (1-2/3)^2 + (-1-2/3)^2]} = \\sqrt{\\frac{1}{3}[\\frac{16}{9} + \\frac{1}{9} + \\frac{25}{9}]} = \\sqrt{\\frac{14}{9}} = \\frac{\\sqrt{14}}{3}$\n",
        "Normalized advantages: $A = \\frac{r - \\bar{r}}{\\sigma}$\n",
        "- $A_1 = \\frac{2 - 2/3}{\\sqrt{14}/3} = \\frac{4/3}{\\sqrt{14}/3} = \\frac{4}{\\sqrt{14}} \\approx 1.069$\n",
        "- $A_2 = \\frac{1 - 2/3}{\\sqrt{14}/3} = \\frac{1/3}{\\sqrt{14}/3} = \\frac{1}{\\sqrt{14}} \\approx 0.267$\n",
        "- $A_3 = \\frac{-1 - 2/3}{\\sqrt{14}/3} = \\frac{-5/3}{\\sqrt{14}/3} = \\frac{-5}{\\sqrt{14}} \\approx -1.336$</font>\n",
        "\n",
        "3. Midway through training you double $\\beta$ in the GRPO loss. Describe the effect this has on the update dynamics.\n",
        "<font color=\"blue\">Doubling $\\beta$ increases the weight of the KL penalty term $\\beta \\, \\mathrm{KL}(\\pi_\\theta \\| \\pi_{\\text{ref}})$, which makes the policy updates more conservative. The model will stay closer to the reference policy, taking smaller steps in parameter space. This reduces the risk of catastrophic forgetting and mode collapse but may slow down learning from the reward signal.</font>\n",
        "\n",
        "4. Starting from $r_t = \\exp(\\log p_\\theta - \\log p_{\\text{ref}})$, write the clipped surrogate used by GRPO and explain how the clipping interacts with the sign of the advantage.\n",
        "<font color=\"blue\">The clipped surrogate is:\n",
        "$$L_{\\text{clip}} = \\min(r_t A_t, \\, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t)$$\n",
        "When $A_t > 0$ (good action), we want to increase its probability, but clipping $r_t$ at $1+\\epsilon$ prevents excessively large updates even if $r_t$ is very large.\n",
        "When $A_t < 0$ (bad action), we want to decrease its probability, but clipping $r_t$ at $1-\\epsilon$ prevents excessively large decreases even if $r_t$ is very small.\n",
        "The min operation selects the more conservative of the two terms, ensuring stable updates.</font>\n",
        "\n",
        "5. Format rewards are dense but answer-correctness rewards are sparse. Propose a shaping scheme that keeps correctness learning signal meaningful and justify it.\n",
        "<font color=\"blue\">Use a two-stage weighted scheme: $r_{\\text{total}} = r_{\\text{format}} + \\lambda \\cdot r_{\\text{format}} \\cdot r_{\\text{correctness}}$, where $\\lambda > 1$ is a scaling factor (e.g., 5-10). This gives a base reward for correct formatting, but multiplies the correctness reward by the format reward, so correctness only contributes when the format is correct. This encourages the model to first learn the format (dense signal) while preserving the learning signal for correctness (sparse but amplified). The multiplicative structure ensures correctness doesn't reinforce incorrect formats.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa14098",
      "metadata": {
        "id": "faa14098"
      },
      "source": [
        "### 2.b GRPO Implementation [30 pts]\n",
        "\n",
        "Complete the reward shaping helpers, grouped-advantage computation, GRPO loss, and the short training/eval loop. Tests in Section 2 cover each of these components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9074abbc",
      "metadata": {
        "id": "9074abbc"
      },
      "source": [
        "### Section 2 implementation guide\n",
        "\n",
        "We now pivot to GRPO fine-tuning. The next block of cells loads libraries, defines the system & user prompts, constructs the reward function, and wires together helper utilities (log-prob extraction, grouped advantages, optimizer setup). Treat those as scaffolding so you can focus on the GRPO-specific TODOs and diagnostics. In particular, the device/import setup, dataset builder, model + LoRA configuration, and format/decoding helper cells are boilerplate—you do not need to edit them.\n",
        "\n",
        "Expect to run the provided GRPO loop on GPU; with the given reward shaping it can drive format adherence close to 100% after enough steps. Format adherence thresholds are tiered: 50% post-RL earns 2 pts, 70% earns 4 pts, and 90% earns the full 6 pts, so feel free to keep training until you hit the ceiling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fff39e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66fff39e",
        "outputId": "d3cc13fe-1d9c-4f10-92e7-7737945ed3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "from collections import deque\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e6cdf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4e6cdf0",
        "outputId": "a66858a0-cb86-492b-ffff-c8837c8e64fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': ['If you add 91 and 2, what is the result?',\n",
              "  'If you add 13 and 5, what is the result?',\n",
              "  'If you add 41 and 4, what is the result?'],\n",
              " 'answer': ['93', '18', '45']}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful assistant. First reason internally, then provide the user answer. \"\n",
        "    \"Respond using the template:\\n\"\n",
        "    \"<think>\\n\"\n",
        "    \"...\\n\"\n",
        "    \"</think>\\n\"\n",
        "    \"<answer>\\n\"\n",
        "    \"...\\n\"\n",
        "    \"</answer>\"\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class QAExample:\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "def build_dataset(num_examples: int = 64) -> Dataset:\n",
        "    records = []\n",
        "    for _ in range(num_examples):\n",
        "        a = random.randint(10, 99)\n",
        "        b = random.randint(1, 9)\n",
        "        question = f\"If you add {a} and {b}, what is the result?\"\n",
        "        answer = str(a + b)\n",
        "        records.append({\"question\": question, \"answer\": answer})\n",
        "    return Dataset.from_list(records)\n",
        "\n",
        "train_ds = build_dataset()\n",
        "train_ds[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc40dc01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc40dc01",
        "outputId": "848ab7b1-ed31-4cb4-f842-34f6d50d50d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model = model.to(device)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02c444fb",
      "metadata": {
        "id": "02c444fb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "FORMAT_PATTERN = re.compile(r\"^<think>\\n[\\s\\S]*\\n</think>\\n<answer>\\n[\\s\\S]*\\n</answer>$\", re.MULTILINE)\n",
        "\n",
        "def format_reward(responses: List[str]) -> torch.Tensor:\n",
        "    scores = [1.0 if FORMAT_PATTERN.match(resp.strip()) else 0.0 for resp in responses]\n",
        "    return torch.tensor(scores, dtype=torch.float32, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "format-reward-sanity",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "format-reward-sanity",
        "outputId": "746f2476-7156-49b2-bcb4-00586ed0748d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "format_reward scores: [1.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "# Sanity checks for format_reward\n",
        "valid_resp = \"<think>\\nreasoning goes here\\n</think>\\n<answer>\\nthe meaning of life is\\n</answer>\"\n",
        "invalid_resp = \"<think>missing closing tags\"\n",
        "scores = format_reward([valid_resp, invalid_resp])\n",
        "print('format_reward scores:', scores.cpu().tolist())\n",
        "assert scores[0].item() == 1.0\n",
        "assert scores[1].item() == 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39001330",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "39001330",
        "outputId": "dc2eb0fe-7c57-4cb8-82ce-8c3e6dcebf1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|im_start|>system\\nYou are a helpful assistant. First reason internally, then provide the user answer. Respond using the template:\\n<think>\\n...\\n</think>\\n<answer>\\n...\\n</answer><|im_end|>\\n<|im_start|>user\\nIf you add 91 and 2, what is the result?<|im_end|>\\n<|im_start|>assistant\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "def build_prompt(question: str) -> str:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "prompts = [build_prompt(rec[\"question\"]) for rec in train_ds]\n",
        "prompts[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6c6a57",
      "metadata": {
        "id": "ee6c6a57"
      },
      "outputs": [],
      "source": [
        "def completion_log_probs(model, sequences: torch.Tensor, prompt_lengths: torch.Tensor) -> torch.Tensor:\n",
        "    attention_mask = (sequences != tokenizer.pad_token_id).long()\n",
        "    outputs = model(input_ids=sequences, attention_mask=attention_mask)\n",
        "    logits = outputs.logits[:, :-1, :]\n",
        "    target_ids = sequences[:, 1:]\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    token_logps = torch.gather(log_probs, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
        "    cont_logps = []\n",
        "    for i in range(sequences.size(0)):\n",
        "        prompt_len = prompt_lengths[i].item()\n",
        "        total_len = attention_mask[i].sum().item()\n",
        "        start = max(prompt_len - 1, 0)\n",
        "        end = max(total_len - 1, start)\n",
        "        cont_logps.append(token_logps[i, start:end].sum())\n",
        "    return torch.stack(cont_logps)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_group(\n",
        "    model,\n",
        "    prompts: List[str],\n",
        "    num_generations: int = 2,\n",
        "    max_new_tokens: int = 128,\n",
        "    do_sample: bool = True,\n",
        "    temperature: float = 0.9,\n",
        "):\n",
        "    encoded = tokenizer(prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
        "    prompt_lengths = encoded[\"attention_mask\"].sum(dim=1)\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    gen_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"do_sample\": do_sample,\n",
        "        \"num_return_sequences\": num_generations,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    }\n",
        "    if do_sample:\n",
        "        gen_kwargs[\"temperature\"] = temperature\n",
        "\n",
        "    outputs = model.generate(**encoded, **gen_kwargs)\n",
        "\n",
        "    batch_size = len(prompts)\n",
        "    sequences = outputs.view(batch_size, num_generations, -1)\n",
        "    seq_list = []\n",
        "    prompt_len_list = []\n",
        "    texts = []\n",
        "    for i in range(batch_size):\n",
        "        prompt_len = prompt_lengths[i].item()\n",
        "        for g in range(num_generations):\n",
        "            ids = sequences[i, g]\n",
        "            mask = (ids != tokenizer.pad_token_id).long()\n",
        "            seq_len = mask.sum().item()\n",
        "            seq_list.append(ids)\n",
        "            prompt_len_list.append(prompt_len)\n",
        "            text_tokens = ids[prompt_len:seq_len]\n",
        "            texts.append(tokenizer.decode(text_tokens, skip_special_tokens=True))\n",
        "    stacked = torch.stack(seq_list).to(device)\n",
        "    prompt_len_tensor = torch.tensor(prompt_len_list, device=device)\n",
        "    rewards = format_reward(texts)\n",
        "    return stacked, prompt_len_tensor, rewards, texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f42a9d6",
      "metadata": {
        "id": "7f42a9d6"
      },
      "outputs": [],
      "source": [
        "def compute_group_advantages(rewards: torch.Tensor, group_size: int) -> torch.Tensor:\n",
        "    if group_size <= 0:\n",
        "        raise ValueError(f\"group_size must be positive; got {group_size}\")\n",
        "    num_rewards = rewards.numel()\n",
        "    if num_rewards % group_size != 0:\n",
        "        raise ValueError(\n",
        "            f\"Reward tensor of length {num_rewards} is not divisible by group_size={group_size}\"\n",
        "        )\n",
        "    reshaped = rewards.view(-1, group_size)\n",
        "    mean = reshaped.mean(dim=1, keepdim=True)\n",
        "    std = reshaped.std(dim=1, keepdim=True, unbiased=False).clamp_min(1e-6)\n",
        "    normalized = (reshaped - mean) / std\n",
        "    return normalized.view(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1197018",
      "metadata": {
        "id": "d1197018"
      },
      "source": [
        "#### GRPO clipped surrogate with KL anchor\n",
        "\n",
        "For each sampled sequence we form\n",
        "$$r_t = \\exp(\\log p_\\theta - \\log p_{\\text{ref}}), \\qquad L_{\\text{clip}} = \\min(r_t A_t, \\mathrm{clip}(r_t, 1\\pm\\epsilon) A_t).$$\n",
        "The full loss subtracts a KL penalty against the frozen reference policy:\n",
        "$$\\mathcal{L}_{\\text{GRPO}} = L_{\\text{clip}} - \\beta\\,\\mathrm{KL}(\\pi_\\theta\\,\\|\\,\\pi_{\\text{ref}}).$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c428c1f",
      "metadata": {
        "id": "9c428c1f"
      },
      "outputs": [],
      "source": [
        "def grpo_sequence_loss(logp_new: torch.Tensor, logp_ref: torch.Tensor, advantages: torch.Tensor, beta: float, epsilon: float) -> torch.Tensor:\n",
        "    # student code here\n",
        "    # TODO: compute the clipped PPO-style surrogate and subtract the GRPO KL penalty term\n",
        "    ratio = torch.exp(logp_new - logp_ref)\n",
        "    clip_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
        "    surr1 = ratio * advantages\n",
        "    surr2 = clip_ratio * advantages\n",
        "    clip_obj = torch.min(surr1, surr2)\n",
        "\n",
        "    # KL divergence using Schulman (2020) estimator: r - log(r) - 1\n",
        "    # where r = π_ref / π_θ\n",
        "    kl_ratio = torch.exp(logp_ref - logp_new)\n",
        "    kl = kl_ratio - torch.log(kl_ratio) - 1\n",
        "\n",
        "    loss = -(clip_obj - beta * kl).mean()\n",
        "    # end student code\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba67280",
      "metadata": {
        "id": "fba67280"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "beta = 0.05\n",
        "epsilon = 0.2\n",
        "group_size = 2\n",
        "\n",
        "\n",
        "def grpo_update(batch_prompts: List[str], grad_accum: int | None = None):\n",
        "    accum_steps = grad_accum if grad_accum is not None else 1\n",
        "    if accum_steps < 1:\n",
        "        raise ValueError(\"grad_accum must be >= 1\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    all_rewards: list[torch.Tensor] = []\n",
        "    all_losses: list[torch.Tensor] = []\n",
        "    collected_texts: list[str] = []\n",
        "\n",
        "    for _ in range(accum_steps):\n",
        "        sequences, prompt_lens, rewards, texts = sample_group(\n",
        "            model,\n",
        "            batch_prompts,\n",
        "            num_generations=group_size,\n",
        "        )\n",
        "\n",
        "        logp_new = completion_log_probs(model, sequences, prompt_lens)\n",
        "\n",
        "        adapters_supported = hasattr(model, \"disable_adapter\") and hasattr(model, \"enable_adapter\")\n",
        "        if adapters_supported:\n",
        "            model.disable_adapter()\n",
        "        with torch.no_grad():\n",
        "            logp_ref = completion_log_probs(model, sequences, prompt_lens)\n",
        "        if adapters_supported:\n",
        "            model.enable_adapter()\n",
        "\n",
        "        advantages = compute_group_advantages(rewards, group_size)\n",
        "        base_loss = grpo_sequence_loss(logp_new, logp_ref, advantages, beta=beta, epsilon=epsilon)\n",
        "        (base_loss / accum_steps).backward()\n",
        "\n",
        "        all_losses.append(base_loss.detach())\n",
        "        all_rewards.append(rewards.detach())\n",
        "        collected_texts.extend(texts)\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    reward_tensor = torch.cat(all_rewards)\n",
        "    reward_mean = reward_tensor.mean().item()\n",
        "    reward_std = reward_tensor.std().item() if reward_tensor.numel() > 1 else 0.0\n",
        "    loss_value = torch.stack(all_losses).mean().item()\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss_value,\n",
        "        \"reward_mean\": reward_mean,\n",
        "        \"reward_std\": reward_std,\n",
        "        \"format_match_pct\": reward_mean * 100.0,\n",
        "        \"texts\": collected_texts,\n",
        "        \"optimizer_step\": True,\n",
        "        \"accumulated_microbatches\": accum_steps,\n",
        "        \"grad_accum_steps\": accum_steps,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1572e1af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1572e1af",
        "outputId": "612f3d88-15fa-4e86-ae8f-011b092a6de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Group advantage mean: max diff=0.00e+00\n",
            "[OK] Group advantage std: max diff=0.00e+00\n",
            "[OK] GRPO loss test: max diff=1.49e-08\n",
            "[OK] GRPO loss backward pass: grad norm=0.4256\n",
            "Section 2 helper tests passed.\n"
          ]
        }
      ],
      "source": [
        "# Unit tests for helpers\n",
        "\n",
        "def check_tensor_close(name: str, actual: torch.Tensor, expected: torch.Tensor, atol: float = 1e-6):\n",
        "    diff = torch.max(torch.abs(actual - expected)).item()\n",
        "    if diff > atol:\n",
        "        raise AssertionError(f\"{name} mismatch (max |Δ|={diff:.3e}). Expected {expected}, got {actual}\")\n",
        "    print(f\"[OK] {name}: max diff={diff:.2e}\")\n",
        "\n",
        "raw_rewards = torch.tensor([1.0, 0.0, 2.0, 4.0], device=device)\n",
        "adv = compute_group_advantages(raw_rewards, group_size=2)\n",
        "reshaped_adv = adv.view(2, 2)\n",
        "check_tensor_close(\"Group advantage mean\", reshaped_adv.mean(dim=1), torch.zeros(2, device=device))\n",
        "check_tensor_close(\"Group advantage std\", reshaped_adv.std(dim=1, unbiased=False), torch.ones(2, device=device))\n",
        "\n",
        "logp_new = torch.log(torch.tensor([0.6, 0.4, 0.7, 0.3], device=device))\n",
        "logp_ref = torch.log(torch.tensor([0.5, 0.5, 0.6, 0.4], device=device))\n",
        "manual_adv = torch.tensor([1.2, -0.4, 0.5, -1.0], device=device)\n",
        "loss = grpo_sequence_loss(logp_new, logp_ref, manual_adv, beta=0.1, epsilon=0.2)\n",
        "check_tensor_close(\"GRPO loss test\", loss, -0.2233469)\n",
        "\n",
        "logp_new_grad = torch.log(torch.tensor([0.55, 0.45, 0.65, 0.35], device=device))\n",
        "logp_new_grad.requires_grad_(True)\n",
        "loss_grad = grpo_sequence_loss(logp_new_grad, logp_ref, manual_adv, beta=0.05, epsilon=0.2)\n",
        "loss_grad.backward()\n",
        "if not torch.all(torch.isfinite(logp_new_grad.grad)):\n",
        "    raise AssertionError(\"Non-finite gradients detected in GRPO loss\")\n",
        "print(f\"[OK] GRPO loss backward pass: grad norm={logp_new_grad.grad.norm().item():.4f}\")\n",
        "\n",
        "print(\"Section 2 helper tests passed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "format-eval",
      "metadata": {
        "id": "format-eval"
      },
      "outputs": [],
      "source": [
        "def evaluate_format_rate(\n",
        "    model,\n",
        "    dataset,\n",
        "    num_examples: int | None = 32,\n",
        "    num_generations: int = 1,\n",
        "    max_new_tokens: int = 128,\n",
        "    do_sample: bool = False,\n",
        "):\n",
        "    if num_examples is None:\n",
        "        subset = dataset\n",
        "    else:\n",
        "        size = min(num_examples, len(dataset))\n",
        "        subset = dataset.select(range(size))\n",
        "    effective_generations = num_generations\n",
        "    if not do_sample and num_generations != 1:\n",
        "        effective_generations = 1\n",
        "    prompts = [build_prompt(ex[\"question\"]) for ex in subset]\n",
        "    _, _, rewards, texts = sample_group(\n",
        "        model,\n",
        "        prompts,\n",
        "        num_generations=effective_generations,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "    )\n",
        "    total = rewards.numel()\n",
        "    matches = rewards.sum().item()\n",
        "    rate = (matches / total) * 100 if total else 0.0\n",
        "    print(f\"Format adherence: {rate:.1f}%\")\n",
        "    return rate, texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a05ffad",
      "metadata": {
        "id": "6a05ffad"
      },
      "source": [
        "### Format evaluation and demo training loop\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "After defining the helpers, we first measure baseline format adherence, then run a short GRPO training loop. Aim for at least 90% post-training adherence to receive full credit (the performance can climb near 100% after 25 steps if done correctly). Training should take approximately 30 minutes in Colab with a T4 GPU.\n",
        "\n",
        "Again, 50% post-RL earns 2 pts, 70% earns 4 pts, and 90% earns the full 6 pts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6a6f36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c6a6f36",
        "outputId": "3f0f93fc-8a5a-4661-a077-65bac4fc9db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline format adherence (pre-RL):\n",
            "Format adherence: 0.0%\n"
          ]
        }
      ],
      "source": [
        "history = deque(maxlen=10)\n",
        "eval_subset = train_ds.select(range(min(32, len(train_ds))))\n",
        "print(\"Baseline format adherence (pre-RL):\")\n",
        "baseline_rate, _ = evaluate_format_rate(\n",
        "    model,\n",
        "    eval_subset,\n",
        "    num_examples=None,\n",
        "    num_generations=group_size,\n",
        "    do_sample=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a036a683",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a036a683",
        "outputId": "d2401857-75c1-4cb0-87c3-c00cd93133cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 01 | reward_mean=0.000 | format adherence=0.0% | time=47.3s\n",
            "Step 02 | reward_mean=0.000 | format adherence=0.0% | time=107.7s\n",
            "Step 03 | reward_mean=0.047 | format adherence=4.7% | time=158.3s\n",
            "Step 04 | reward_mean=0.000 | format adherence=0.0% | time=219.2s\n",
            "Step 05 | reward_mean=0.062 | format adherence=6.2% | time=270.4s\n",
            "Step 06 | reward_mean=0.047 | format adherence=4.7% | time=322.7s\n",
            "Step 07 | reward_mean=0.141 | format adherence=14.1% | time=370.7s\n",
            "Step 08 | reward_mean=0.172 | format adherence=17.2% | time=417.1s\n",
            "Step 09 | reward_mean=0.219 | format adherence=21.9% | time=462.2s\n",
            "Step 10 | reward_mean=0.266 | format adherence=26.6% | time=505.4s\n",
            "Step 11 | reward_mean=0.328 | format adherence=32.8% | time=545.3s\n",
            "Step 12 | reward_mean=0.375 | format adherence=37.5% | time=590.2s\n",
            "Step 13 | reward_mean=0.297 | format adherence=29.7% | time=639.7s\n",
            "Step 14 | reward_mean=0.422 | format adherence=42.2% | time=687.8s\n",
            "Step 15 | reward_mean=0.500 | format adherence=50.0% | time=721.5s\n",
            "Step 16 | reward_mean=0.562 | format adherence=56.2% | time=758.3s\n",
            "Step 17 | reward_mean=0.422 | format adherence=42.2% | time=818.5s\n",
            "Step 18 | reward_mean=0.656 | format adherence=65.6% | time=854.6s\n",
            "Step 19 | reward_mean=0.703 | format adherence=70.3% | time=898.6s\n",
            "Step 20 | reward_mean=0.719 | format adherence=71.9% | time=943.7s\n",
            "Step 21 | reward_mean=0.906 | format adherence=90.6% | time=985.9s\n",
            "Step 22 | reward_mean=0.984 | format adherence=98.4% | time=1030.3s\n",
            "Step 23 | reward_mean=0.969 | format adherence=96.9% | time=1061.8s\n",
            "Step 24 | reward_mean=0.938 | format adherence=93.8% | time=1109.7s\n",
            "Step 25 | reward_mean=1.000 | format adherence=100.0% | time=1145.2s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "for step in range(25):\n",
        "    batch = train_ds.shuffle(seed=step).select(range(4))  # batch size 4\n",
        "    prompts_batch = [build_prompt(ex[\"question\"]) for ex in batch]\n",
        "    stats = grpo_update(prompts_batch, grad_accum=8)  # simulate batch size 4x8=32\n",
        "    history.append(stats[\"reward_mean\"])\n",
        "    print(\n",
        "        f\"Step {step + 1:02d} | reward_mean={stats['reward_mean']:.3f} | format adherence={stats['format_match_pct']:.1f}% | time={(time.time() - start_time):.1f}s\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3827bcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3827bcf",
        "outputId": "594cdf4e-28a4-4a7e-db75-cc9da1a11863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Format adherence after RL:\n",
            "Format adherence: 100.0%\n",
            "Baseline: 0.0% | Post-RL: 100.0%\n"
          ]
        }
      ],
      "source": [
        "print(\"Format adherence after RL:\")\n",
        "post_rate, _ = evaluate_format_rate(\n",
        "    model,\n",
        "    eval_subset,\n",
        "    num_examples=None,\n",
        "    num_generations=group_size,\n",
        "    do_sample=False,\n",
        ")\n",
        "print(f\"Baseline: {baseline_rate:.1f}% | Post-RL: {post_rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae9805d",
      "metadata": {
        "id": "5ae9805d"
      },
      "source": [
        "## Submission Checklist\n",
        "\n",
        "- All answers filled.\n",
        "- Section 1 tests succeed; visualization optional.\n",
        "- Section 2 helper tests succeed; optional demo loop left commented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96bc76d1",
      "metadata": {
        "id": "96bc76d1"
      },
      "source": [
        "### Chain-of-Thought Playground Example\n",
        "\n",
        "Use this cell to poke the fine-tuned model with your own prompts. We start with a simple 4-digit addition problem so you can see the expected <think>/<answer> chain of thought. Edit `example_question` and re-run as you like.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4342a3e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4342a3e8",
        "outputId": "d798c404-1ca5-46b3-bcb4-f6d6d6510a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What's 4821+11?\n",
            "Model completion:\n",
            "<think>\n",
            "To solve 4821 + 11, you simply add them together.\n",
            "</think>\n",
            "<answer>\n",
            "4932\n",
            "</answer>\n"
          ]
        }
      ],
      "source": [
        "example_question = \"What's 4821+11?\"\n",
        "example_prompt = build_prompt(example_question)\n",
        "print(\"Prompt:\", example_question)\n",
        "\n",
        "inputs = tokenizer(example_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False).to(device)\n",
        "input_len = inputs[\"input_ids\"].shape[1]\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "new_tokens = output_ids[0, input_len:]\n",
        "completion = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "print(\"Model completion:\")\n",
        "print(completion.strip())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}